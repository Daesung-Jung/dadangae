{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "execute_gpu.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daesung-Jung/baseball_pitchdesign/blob/main/execute_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T78JaokPA-jV"
      },
      "source": [
        "#1. 환경설정 확인\n",
        ">  GPU 확인, \n",
        "깃허브 private clone "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPpixvtVjpkY"
      },
      "source": [
        "#코랩 GPU 확인\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        " # 추가 메모리 확인 \n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ56oDYe7mBJ"
      },
      "source": [
        "#import torch\n",
        "#torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaBB7rF8j0TC"
      },
      "source": [
        "!ssh-keygen -t rsa -b 4096 -C \"shtnrgudsla@naver.com\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Em8O8HTMajY"
      },
      "source": [
        "!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUK4b0JZPHmm"
      },
      "source": [
        "> 아래 나오는 키 복사해서 깃허브 계정 내 SSH~ 어쩌고 내용에 저장. title은 암거나."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtYbVNenOLCy"
      },
      "source": [
        "!cat /root/.ssh/id_rsa.pub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CoYOWRyONIi"
      },
      "source": [
        "!ssh -T git@github.com\n",
        "\n",
        "!git config --global user.email \"shtnrgudsla@naver.com\"\n",
        "!git config --global user.name \"wjd890708!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSyUeZ6CPi65"
      },
      "source": [
        "# 2. git clone "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdsiQjnVATkP"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#%cd /content/drive/MyDrive/\n",
        "# 주소대신 ssh copy \n",
        "#!git clone git@github.com:Daesung-Jung/baseball_pitchdesign.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kW5a71sk0t9"
      },
      "source": [
        "# 존재한다면 최신화\n",
        "%cd /content/drive/MyDrive/baseball_pitchdesign/\n",
        "\n",
        "#!git remote add origin /content/drive/MyDrive/baseball_pitchdesign/\n",
        "!git reset --hard HEAD\n",
        "!git pull\n",
        "\n",
        "#!git pull \n",
        "#필요한 패키지 설치\n",
        "%cd /content/drive/MyDrive/baseball_pitchdesign/\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0VeRDbPyu6Z"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TWQua8vBzZv"
      },
      "source": [
        "import yaml\n",
        "%cd /\n",
        "from glob import glob\n",
        "\n",
        "img_list = glob('/content/drive/MyDrive/baseball_pitchdesign/images/*.jpg')\n",
        "lab_list = glob('/content/drive/MyDrive/baseball_pitchdesign/labels/*.txt')\n",
        "\n",
        "print(len(img_list))\n",
        "print(len(lab_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnFgfq-OmPIR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_img_list, val_img_list = train_test_split(img_list, test_size=0.2, random_state=2000)\n",
        "\n",
        "print(len(train_img_list), len(val_img_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW6HC3HAmTmF"
      },
      "source": [
        "with open('/content/drive/MyDrive/baseball_pitchdesign/train.txt', 'w') as f:\n",
        "  f.write('\\n'.join(train_img_list) + '\\n')\n",
        "\n",
        "with open('/content/drive/MyDrive/baseball_pitchdesign/val.txt', 'w') as f:\n",
        "  f.write('\\n'.join(val_img_list) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_8qNSws4xVU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIUvbzCNmhb7"
      },
      "source": [
        "import yaml\n",
        "\n",
        "with open('/content/drive/MyDrive/baseball_pitchdesign/data.yaml', 'r') as f:\n",
        "  data = yaml.load(f)\n",
        "\n",
        "print(data)\n",
        "data['train'] = '/content/drive/MyDrive/baseball_pitchdesign/train.txt'\n",
        "data['val'] = '/content/drive/MyDrive/baseball_pitchdesign/val.txt'\n",
        "\n",
        "print(data['val'])\n",
        "with open('/content/drive/MyDrive/baseball_pitchdesign/data.yaml', 'w') as f:\n",
        "  yaml.dump(data, f)\n",
        "\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu-q7AvZH33F"
      },
      "source": [
        "# 3. 학습하기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNPVHoFcCVOO"
      },
      "source": [
        "%cd /content/drive/MyDrive/baseball_pitchdesign/yolov5/\n",
        "!python train.py --img 416 --batch 32 --epochs 600 --data /content/drive/MyDrive/baseball_pitchdesign/data.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt --name elbow_bg_all_32_600"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH7OUI-NIJcd"
      },
      "source": [
        "# 4. 결과 비교"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bPAB6L1UHEJ"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/baseball_pitchdesign/yolov5/runs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nLUMK1pq2re"
      },
      "source": [
        "file_list_[144]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgIAho7jINSw"
      },
      "source": [
        "\n",
        "# 5. 실제 데이터 넣기\n",
        "\n",
        "> 원하는 만큼 input, output 숫자로 넣기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr1QdzM-xRXv"
      },
      "source": [
        "import os\n",
        "\n",
        "%cd /content/drive/MyDrive/baseball_pitchdesign/yolov5/\n",
        "\n",
        "\n",
        "def cap_slomo(path = \"/content/drive/MyDrive/project_ddg/rawdata_video/2/\"):\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "  path = path\n",
        "  file_list_ = os.listdir(path)\n",
        "  file_list_ = sorted(file_list_)\n",
        "  print(\"슬로우모션을 뽑아낼 데이터, [1~\",len(file_list_),\"번째] 중\"\n",
        "  \"\\n처음(start)과 마지막(last)를 <숫자만> 입력하세요\" )\n",
        "  start = int(input(\"처음 : \"))\n",
        "  #print(\"처음 : \",start)\n",
        "  end = int(input(\"마지막 : \"))\n",
        "\n",
        "\n",
        "  for aa in range(start-1,end):\n",
        "\n",
        "    print(aa+1, file_list_[aa], \"실행합니다\")\n",
        "    #변수명을 직접 넣으면 안돼서 {}활용 \n",
        "    file_name_=file_list_[aa]\n",
        "    !python detect.py --weight /content/drive/MyDrive/baseball_pitchdesign/weight/elbow_bg_all_plus_600.pt --img 416 --conf 0.9 --source '{path}{file_name_}' --nosave\n",
        "    \n",
        "#괄호안에 해당 디렉토리를 넣어주세요. 디폴트는 2로 설정!\n",
        "\n",
        "#가령 윤수는 cap_slomo(\"/content/drive/MyDrive/rawdata_video/3/\")\n",
        "#안 넣으면 디폴트\n",
        "\n",
        "cap_slomo()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLe6ePxApI-b"
      },
      "source": [
        "import os\n",
        "\n",
        "%cd /content/drive/MyDrive/baseball_pitchdesign/yolov5/\n",
        "\n",
        "\n",
        "def cap_slomo(path = \"/content/drive/MyDrive/project_ddg/rawdata_video/2/\"):\n",
        "       \n",
        "  path = path\n",
        "  file_list_ = os.listdir(path)\n",
        "  file_list_ = sorted(file_list_)\n",
        "  print(\"슬로우모션을 뽑아낼 데이터, [1~\",len(file_list_),\"번째] 중\"\n",
        "  \"\\n처음(start)과 마지막(last)를 <숫자만> 입력하세요\" )\n",
        "  start = int(input(\"처음 : \"))\n",
        "  #print(\"처음 : \",start)\n",
        "  end = int(input(\"마지막 : \"))\n",
        "\n",
        "\n",
        "  for aa in range(start-1,end):\n",
        "\n",
        "    print(aa+1, file_list_[aa], \"실행합니다\")\n",
        "    #변수명을 직접 넣으면 안돼서 {}활용 \n",
        "    file_name_=file_list_[aa]\n",
        "    !python detect_re.py --weight /content/drive/MyDrive/baseball_pitchdesign/weight/elbow_bg_all_plus_600.pt --img 416 --conf 0.9 --source '{path}{file_name_}' --nosave\n",
        "    \n",
        "#괄호안에 해당 디렉토리를 넣어주세요. 디폴트는 2로 설정!\n",
        "\n",
        "#가령 윤수는 cap_slomo(\"/content/drive/MyDrive/rawdata_video/3/\")\n",
        "#안 넣으면 디폴트\n",
        "\n",
        "cap_slomo()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqW1LBmExbfx"
      },
      "source": [
        "#수동으로 작업 \n",
        "\n",
        "%cd /content/drive/MyDrive/baseball_pitchdesign/yolov5/\n",
        "!python detect.py --weight /content/drive/MyDrive/baseball_pitchdesign/weight/elbow_bg_all_plus_600.pt --img 416 --conf 0.8 --source /content/drive/MyDrive/dataset/20210414KTOB02021-2.mp4 --nosave"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCtLAAmvKto-"
      },
      "source": [
        "pip uninstall lightning-flash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A-6QgswDT79"
      },
      "source": [
        "#!git clone https://github.com/PyTorchLightning/lightning-flash.git\n",
        "!pip install pytorch-lightning\n",
        "!pip install lightning-flash\n",
        "!pip install kornia\n",
        "!pip install pytorchvideo\n",
        "#!pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUgzgfD3LBgd"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/lightning-flash\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlIHZBglDKj9"
      },
      "source": [
        "import os\n",
        "from typing import Callable, List\n",
        "import pytorch_lightning\n",
        "import pytorchvideo.data\n",
        "import torch.utils.data\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "\n",
        "import kornia.augmentation as K\n",
        "import torch\n",
        "from pytorchvideo.transforms import ApplyTransformToKey, RandomShortSideScale, UniformTemporalSubsample\n",
        "from torch.utils.data.sampler import RandomSampler\n",
        "from torchvision.transforms import CenterCrop, Compose, RandomCrop, RandomHorizontalFlip\n",
        "\n",
        "import flash\n",
        "from flash.core.finetuning import NoFreeze\n",
        "from flash.core.classification import Labels\n",
        "from flash.video import VideoClassificationData, VideoClassifier\n",
        "\n",
        "from flash.data.utils import download_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Eg9WhMOMlPW"
      },
      "source": [
        "\n",
        "download_data(\"https://pl-flash-data.s3.amazonaws.com/kinetics.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypBTOem7DbRt"
      },
      "source": [
        "# 2. [Optional] Specify transforms to be used during training.\n",
        "post_tensor_transform = [UniformTemporalSubsample(8), RandomShortSideScale(min_size=256, max_size=320)]\n",
        "\n",
        "per_batch_transform_on_device = [K.Normalize(torch.tensor([0.45, 0.45, 0.45]), torch.tensor([0.225, 0.225, 0.225]))]\n",
        "\n",
        "train_post_tensor_transform = post_tensor_transform + [RandomCrop(244), RandomHorizontalFlip(p=0.5)]\n",
        "\n",
        "val_post_tensor_transform = post_tensor_transform + [CenterCrop(244)]\n",
        "train_per_batch_transform_on_device = per_batch_transform_on_device\n",
        "\n",
        "\n",
        "def make_transform(\n",
        "    post_tensor_transform: List[Callable] = post_tensor_transform,\n",
        "    per_batch_transform_on_device: List[Callable] = per_batch_transform_on_device\n",
        "):\n",
        "    return {\n",
        "        \"post_tensor_transform\": Compose([\n",
        "            ApplyTransformToKey(\n",
        "                key=\"video\",\n",
        "                transform=Compose(post_tensor_transform),\n",
        "            ),\n",
        "        ]),\n",
        "        \"per_batch_transform_on_device\": Compose([\n",
        "            ApplyTransformToKey(\n",
        "                key=\"video\",\n",
        "                transform=K.VideoSequential(\n",
        "                  *per_batch_transform_on_device, data_format=\"BCTHW\", same_on_frame=False)\n",
        "            ),\n",
        "        ]),\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqjDHzDcDbyg"
      },
      "source": [
        "\n",
        "# 2. Load the data\n",
        "datamodule = VideoClassificationData.from_folders(\n",
        "    train_folder=\"/content/drive/MyDrive/data/train/\",\n",
        "    val_folder=\"/content/drive/MyDrive/data/val/\",\n",
        "    test_folder=\"/content/drive/MyDrive/data/predict/\",\n",
        "    train_transform=make_transform(train_post_tensor_transform),\n",
        "    val_transform=make_transform(val_post_tensor_transform),\n",
        "    predict_transform=make_transform(val_post_tensor_transform),\n",
        "    batch_size=8,\n",
        "    clip_sampler=\"uniform\",\n",
        "    clip_duration=2,\n",
        "    video_sampler=\"random\",\n",
        "    decode_audio=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHafjsD4dqUd"
      },
      "source": [
        "#6. GCP Storage to Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcio7E3LQhC0"
      },
      "source": [
        "#접근, 인증필요함 \n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'dadangae'\n",
        "!gcloud config set project dadangae\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chEl61UZggKz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix1_3KqNggEj"
      },
      "source": [
        "!gcloud auth login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDPztRaFg_Bg"
      },
      "source": [
        "!gcloud auth list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwm48QKli6Wa"
      },
      "source": [
        "!gcloud config set project dadangae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OeaOBqbd-bd"
      },
      "source": [
        "#실행\n",
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pBlqp8teh9H"
      },
      "source": [
        "2\n",
        "!gsutil ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmdfzUPqQvl2"
      },
      "source": [
        "1%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4VqkGd3FqEL"
      },
      "source": [
        "#gcp to google drive \n",
        "#-m 붙이면 가속화할 수 있으나 파일명 확장자가 조금 이상하게 떨어짐 \n",
        "!gsutil cp -r gs://diego/2/* /content/drive/MyDrive/project_ddg/rawdata_video/2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob6xydXeSWtT"
      },
      "source": [
        "#7. 쉘 언어에서 \n",
        "1. 해당 파일의 리스트를 받기\n",
        "2. 쉘에서 변수명을 실행할 땐 앞엔 $를 붙여야 함  -> 일종의 이스케이프(\\) 개념\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCCzWZ6MTzAt"
      },
      "source": [
        "\n",
        "'''\n",
        "$ gsutil list                           # 나의 버킷 리스트 보기\n",
        "$ gsutil ls -r gs://버킷이름             # 버킷 안에 들어있는 파일 확인\n",
        "$ gsutil du -s gs://버킷이름             # 버킷 용량 확인\n",
        "$ gsutil mb gs://버킷이름                # 버킷 생성\n",
        "$ gsutil rb gs://버킷이름                # 버킷 삭제\n",
        "$ gsutil cp 로컬 파일 위치 gs://버킷이름   # 로컬 -> 버킷 복사\n",
        "$ gsutil cp gs://버킷이름 로컬 파일 위치   # 버킷 -> 로컬 복사\n",
        "$ gsutil mv 로컬 파일 위치 gs://버킷이름   # 로컬 -> 버킷 이동\n",
        "$ gsutil mv gs://버킷이름 로컬 파일 위치   # 버킷 -> 로컬 이동\n",
        "$ gsutil rm gs://버킷이름/파일이름        # 파일 삭제\n",
        "$ gsutil ls -L gs://버킷이름/파일이름     # 파일 정보 보기\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CAuUDKyIxKm"
      },
      "source": [
        "#GCP 해당 폴더 리스트 만들기 \n",
        "df = !gsutil ls -r gs://diego/2\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzpCmnJ4ZCGe"
      },
      "source": [
        "#최상위에 temp_video 폴더를  생성\n",
        "%cd \\\n",
        "%cd ..\n",
        "%mkdir temp_video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91t25IW7gTf4"
      },
      "source": [
        "!pip install firebase_admin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hocEhbtZgYOw"
      },
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import storage\n",
        "import datetime\n",
        "\n",
        "import urllib.request as req\n",
        "import cv2\n",
        "\n",
        "cred = credentials.Certificate('/content/adc.json')\n",
        "app = firebase_admin.initialize_app(cred, {'storageBucket': 'cnc-designs.appspot.com'}, name='storage')\n",
        "bucket = storage.bucket(app=app)\n",
        "\n",
        "def generate_image_url(blob_path):\n",
        "    \"\"\" generate signed URL of a video stored on google storage. \n",
        "        Valid for 300 seconds in this case. You can increase this \n",
        "        time as per your requirement. \n",
        "    \"\"\"                                                        \n",
        "    blob = bucket.blob(blob_path) \n",
        "    return blob.generate_signed_url(datetime.timedelta(seconds=300), method='GET')\n",
        "\n",
        "\n",
        "url = generate_image_url('sample1.mp4')\n",
        "req.urlretrieve(url, \"sample1.mp4\")\n",
        "cap = cv2.VideoCapture('sample1.mp4')\n",
        "\n",
        "if cap.isOpened():\n",
        "    print (\"File Can be Opened\")\n",
        "    while(True):\n",
        "        # Capture frame-by-frame\n",
        "        ret, frame = cap.read()\n",
        "        #print cap.isOpened(), ret\n",
        "        if frame is not None:\n",
        "            # Display the resulting frame\n",
        "            cv2.imshow('frame',frame)\n",
        "            # Press q to close the video windows before it ends if you want\n",
        "            if cv2.waitKey(22) & 0xFF == ord('q'):\n",
        "                break\n",
        "        else:\n",
        "            print(\"Frame is None\")\n",
        "            break\n",
        "    # When everything done, release the capture\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print (\"Video stop\")\n",
        "else:\n",
        "    print(\"Not Working\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3SP_WI1XeLj"
      },
      "source": [
        "\n",
        "%cd  temp_video\n",
        "for aa in range(6,12)):\n",
        "  q=df[aa]\n",
        "  print(q, \"다운로드 중\")\n",
        "  !gsutil cp $q .\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvfEeMGbdLoF"
      },
      "source": [
        "!gsutil cp $q ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QwQhFE9a5K0"
      },
      "source": [
        "!python /content/drive/MyDrive/baseball_pitchdesign/yolov5/detect.py --weight /content/drive/MyDrive/baseball_pitchdesign/weight/elbow_bg_all_plus_600.pt --img 416 --conf 0.9 --source /content/drive/MyDrive/rawdata_video/2/20210414LGWO02021-3.mp4 --nosave"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkGa8NGDbCzN"
      },
      "source": [
        "!python /content/drive/MyDrive/baseball_pitchdesign/yolov5/detect.py --weight /content/drive/MyDrive/baseball_pitchdesign/weight/elbow_bg_all_plus_600.pt --img 416 --conf 0.9 --source !gsutil cp $!gsutil cp $q --nosave"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrS_k72hWqYV"
      },
      "source": [
        "#구글 클라우드 SDK 설치 \n",
        "%cd /content/drive/MyDrive/\n",
        "!curl https://sdk.cloud.google.com | bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR-uiTdQSBmL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyc0tiXDSrYD"
      },
      "source": [
        "#Download file from Cloud Storage to Google Colab\n",
        "#!gsutil cp gs://diego/2/20210414KTOB02021-1.mp4 .\n",
        "!gsutil cp gs://diego/2/20210414KTOB02021-2.mp4 .\n",
        "!gsutil cp gs://diego/2/20210414KTOB02021-3.mp4 .\n",
        "!gsutil cp gs://diego/2/20210414KTOB02021-4.mp4 .\n",
        "!gsutil cp gs://diego/2/20210414LGWO02021-1.mp4 .\n",
        "#24초 걸림 (2.6기가)\n",
        "\n",
        "\n",
        "#Upload file from Google Colab to Cloud\n",
        "#!gsutil cp 20210624NCLT02021-1.mp4 gs://diego/example/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISUomSsQTS3E"
      },
      "source": [
        "#GCP에서 구글 드라이브로 (45초)\n",
        "!gsutil cp gs://diego/1/20210410KTSS02021-2.mp4 /content/drive/MyDrive/20210414KTOB02021-1.mp4\n",
        "!gsutil cp gs://diego/1/20210410KTSS02021-2.mp4 /content/drive/MyDrive/20210414KTOB02021-2.mp4\n",
        "!gsutil cp gs://diego/1/20210410KTSS02021-2.mp4 /content/drive/MyDrive/20210414KTOB02021-3.mp4\n",
        "!gsutil cp gs://diego/1/20210410KTSS02021-2.mp4 /content/drive/MyDrive/20210414KTOB02021-4.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cas1-GEqZpRk"
      },
      "source": [
        "#실제 진행 \n",
        "!python /content/drive/MyDrive/baseball_pitchdesign/yolov5/detect.py --weight /content/drive/MyDrive/baseball_pitchdesign/weight/elbow_bg_all_plus_600.pt --img 416 --conf 0.9 --source 20210414LGWO02021-2.mp4 --nosave\n",
        "!python /content/drive/MyDrive/baseball_pitchdesign/yolov5/detect.py --weight /content/drive/MyDrive/baseball_pitchdesign/weight/elbow_bg_all_plus_600.pt --img 416 --conf 0.9 --source 20210414LGWO02021-3.mp4 --nosave\n",
        "!python /content/drive/MyDrive/baseball_pitchdesign/yolov5/detect.py --weight /content/drive/MyDrive/baseball_pitchdesign/weight/elbow_bg_all_plus_600.pt --img 416 --conf 0.9 --source 20210414LGWO02021-4.mp4 --nosave\n",
        "!python /content/drive/MyDrive/baseball_pitchdesign/yolov5/detect.py --weight /content/drive/MyDrive/baseball_pitchdesign/weight/elbow_bg_all_plus_600.pt --img 416 --conf 0.9 --source 20210414LTHT02021-1.mp4 --nosave\n",
        "\n",
        "!python /content/drive/MyDrive/baseball_pitchdesign/yolov5/detect.py --weight /content/drive/MyDrive/baseball_pitchdesign/weight/elbow_bg_all_plus_600.pt --img 416 --conf 0.9 --source 20210414LTHT02021-2.mp4 --nosave\n",
        "!python /content/drive/MyDrive/baseball_pitchdesign/yolov5/detect.py --weight /content/drive/MyDrive/baseball_pitchdesign/weight/elbow_bg_all_plus_600.pt --img 416 --conf 0.9 --source 20210414LTHT02021-3.mp4 --nosave\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qvi-Hv7afzO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw0x_Qsabmt2"
      },
      "source": [
        "!pip install av\n",
        "! wget https://raw.githubusercontent.com/pytorch/vision/6de158c473b83cf43344a0651d7c01128c7850e6/references/video_classification/transforms.py\n",
        "# Download HMDB51 data and splits from serre lab website\n",
        "! wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n",
        "! wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/test_train_splits.rar\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRF2Soutb562"
      },
      "source": [
        "pip install git+https://github.com/Atze00/MoViNet-pytorch.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iupCYQH7b8HG"
      },
      "source": [
        "# Extract and organize video data..\n",
        "! mkdir -p video_data test_train_splits\n",
        "! unrar e test_train_splits.rar test_train_splits\n",
        "! rm test_train_splits.rar\n",
        "! unrar e hmdb51_org.rar \n",
        "! rm hmdb51_org.rar\n",
        "! mv *.rar video_data\n",
        "import os\n",
        "for files in os.listdir('video_data'):\n",
        "    foldername = files.split('.')[0]\n",
        "    os.system(\"mkdir -p video_data/\" + foldername)\n",
        "    os.system(\"unrar e video_data/\"+ files + \" video_data/\"+foldername)\n",
        "\n",
        "! rm video_data/*.rar\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeNqP_lZkTQL"
      },
      "source": [
        "#8. 구종 동영상 폴더 자동 분류(이동) 및 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qtliz7YcCQM"
      },
      "source": [
        "#우리가 취합한 데이터 파일 \n",
        "import pandas as pd\n",
        "data_list = pd.read_csv(\"/content/drive/MyDrive/project_ddg/total__.csv\")\n",
        "data_list.head()\n",
        "data_list=data_list[['pitcher','pitch_type','gameId','cumsum','no']]\n",
        "data_list.groupby('pitch_type').agg('count')['pitcher']\n",
        "data_list['file_name']= data_list['gameId']+\"_\"+data_list['no'].astype(str)+\".mp4\"\n",
        "data_list['pitch_type'] = data_list['pitch_type'].replace(\"슬라이더\",\"SL\").replace(\"포크\",\"FO\").replace(\"직구\",\"FF\").replace(\"커터\",\"FC\").replace(\"투심\",\"FT\").replace(\"체인지업\",\"CH\").replace(\"커브\",\"CU\")\n",
        "data_list['pitch_type'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3iXJoYWedBc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIoWMsDAs2es"
      },
      "source": [
        "'''\n",
        "#동영상 데이터 복사해서 Class 별로 이동 \n",
        "import os\n",
        "main=\"/content/drive/MyDrive/project_ddg/\"\n",
        "for (path, directory, files) in os.walk(\"/content/drive/MyDrive/project_ddg/zoom_in/videos\"):\n",
        "  for aa in files:   \n",
        "    exist_ = len(data_list[data_list['file_name']==aa])\n",
        "    print(aa)\n",
        "    if exist_!=0:      \n",
        "      print(\"-------------------------------------\"+aa+\"-------------------------------------\")\n",
        "      pitch_type = data_list[data_list['file_name']==aa]['pitch_type']\n",
        "      pitch_type = pitch_type.values[0]\n",
        "      !cp $path\"/\"$aa $main\"class_video/\"$pitch_type\"/\"$aa\n",
        "\n",
        "%cd /content/drive/MyDrive/project_ddg/train_test_split\n",
        "#해당 폴더 내 폴더 생성 \n",
        "!mkdir CH\n",
        "!mkdir CU\n",
        "!mkdir FC\n",
        "!mkdir FF\n",
        "!mkdir FO\n",
        "!mkdir FT\n",
        "!mkdir SL\n",
        "\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_folder(folder_name):\n",
        "  X= os.listdir(\"/content/drive/MyDrive/project_ddg/class_video/\"+str(folder_name))\n",
        "  #7:3 스플릿, 섞기\n",
        "  X_train, X_test = train_test_split(X, test_size=0.3, shuffle=True, random_state=1004)\n",
        "  #파일 이름, 한 줄씩 처리\n",
        "  with open(\"/content/drive/MyDrive/project_ddg/train_test_split/{0}1.txt\".format(str(folder_name)), \"w\") as file:\n",
        "    for aa in X_train:\n",
        "      file.write(\"{0} 1\\n\".format(str(aa)))\n",
        "    for bb in X_test:\n",
        "      file.write(\"{0} 2\\n\".format(str(bb)))  \n",
        "\n",
        "split_folder(\"FF\")\n",
        "split_folder(\"FC\")\n",
        "split_folder(\"FO\")\n",
        "split_folder(\"FT\")\n",
        "split_folder(\"SL\")\n",
        "split_folder(\"CU\")\n",
        "split_folder(\"CH\")\n",
        "\n",
        "\n",
        "#동영상 데이터 복사해서 Class 별로 이동 \n",
        "import os\n",
        "main=\"/content/drive/MyDrive/project_ddg/\"\n",
        "for (path, directory, files) in os.walk(\"project_ddg\"):\n",
        "  for aa in files:   \n",
        "    exist_ = len(data_list[data_list['file_name']==aa])\n",
        "    print(aa)\n",
        "    if exist_!=0:      \n",
        "      print(\"-------------------------------------\"+aa+\"-------------------------------------\")\n",
        "      pitch_type = data_list[data_list['file_name']==aa]['pitch_type']\n",
        "      pitch_type = pitch_type.values[0]\n",
        "      !cp $path\"/\"$aa $main\"class_video/\"$pitch_type\"/\"$aa\n",
        "\n",
        "%cd /content/drive/MyDrive/project_ddg\n",
        "#해당 폴더 내 폴더 생성 \n",
        "!mkdir sort_video\n",
        "\n",
        "#클래스 비디오 안에 있는 구종_파일이름을 sor_vidoe로 \n",
        "import os\n",
        "main=\"/content/drive/MyDrive/project_ddg\"\n",
        "for (path, directory, files) in os.walk(str(main)+\"/class_video\"):\n",
        "  for aa in files:    \n",
        "    new=path.split(\"/\")[-1]+\"_\"+str(aa)\n",
        "    !cp $path\"/\"$aa $main\"/sort_video/\"$new \n",
        "'''\n",
        "\n",
        "%cd /content/drive/MyDrive/project_ddg\n",
        "#해당 폴더 내 폴더 생성 \n",
        "!mkdir videos\n",
        "\n",
        "#동영상 데이터 복사해서 Class 별로 이동 \n",
        "import os\n",
        "main=\"/content/drive/MyDrive/project_ddg/videos\"\n",
        "for (path, directory, files) in os.walk(\"/content/drive/MyDrive/project_ddg/zoom_in/videos\"):\n",
        "  for aa in files:   \n",
        "    exist_ = len(data_list[data_list['file_name']==aa])\n",
        "    print(aa)\n",
        "    if exist_!=0:      \n",
        "      print(\"-------------------------------------\"+aa+\"-------------------------------------\")\n",
        "      pitch_type = data_list[data_list['file_name']==aa]['pitch_type']\n",
        "      pitch_type = pitch_type.values[0]\n",
        "      !cp $main\"/\"$aa $main\"/data/\"$pitch_type\"_\"$aa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbMm4wDAK_QJ"
      },
      "source": [
        "#구종 구분\n",
        "import pandas as pd\n",
        "files = os.listdir( \"/content/drive/MyDrive/project_ddg/videos/\")\n",
        "files = pd.DataFrame(files)\n",
        "files['pitch_type'] = pd.DataFrame(files)[0].str[0:2]\n",
        "\n",
        "#트레인 테스트 나누기\n",
        "for aa in files['pitch_type'].unique():\n",
        "  X =files[files['pitch_type']==aa]\n",
        "  X_train, X_test = train_test_split(X, test_size=0.3, shuffle=True, random_state=1004)\n",
        "  files.loc[files[0].isin(X_train[0]),'type'] = 'train'\n",
        "  files.loc[files[0].isin(X_test[0]),'type'] = 'test'\n",
        "files.columns= [\"video_id\",\"label\",\"type\"]\n",
        "real_file=files\n",
        "\n",
        "%cd /content/drive/MyDrive/project_ddg/videos/\n",
        "#해당 폴더 내 폴더 생성 \n",
        "!mkdir train\n",
        "!mkdir test\n",
        "\n",
        "import os\n",
        "main=\"/content/drive/MyDrive/project_ddg/videos\"\n",
        "for (path, directory, files) in os.walk(str(main)):\n",
        "  for cc in files:    \n",
        "    #파일이름 \n",
        "    folder = real_file[real_file['video_id']==cc]['type'].values[0]\n",
        "    #train test 갈라서 해당 폴더에 넣기\n",
        "    !mv $path\"/\"$cc \"/content/drive/MyDrive/project_ddg/videos/\"$folder\"/\"$cc \n",
        "\n",
        "\n",
        "train_list = files[files['type']==\"train\"]\n",
        "test_list = files[files['type']==\"test\"]\n",
        "\n",
        "del train_list['type']\n",
        "train_list.to_csv(\"/content/drive/MyDrive/project_ddg/videos/train/train_labels.csv\",index=False)\n",
        "\n",
        "del test_list['type']\n",
        "test_list.to_csv(\"/content/drive/MyDrive/project_ddg/videos/test/test_labels.csv\",index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqTh8KNLSzPo"
      },
      "source": [
        "#zip -r ~ ~ 압축하는 데 뒷 경로가 상위 디렉토리가 포함되어 있음 이거 어캐함\n",
        "#이후 삭제 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhOyt2iVhh1T"
      },
      "source": [
        "#9.Data DownLoad\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTKS0Mvyhgs8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#%cd /content/drive/MyDrive/\n",
        "# 주소대신 ssh copy \n",
        "#!git clone git@github.com:Daesung-Jung/baseball_pitchdesign.git\n",
        "\n",
        "!pip install -q tf-models-nightly tfds-nightly\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fHvThm7pF1h"
      },
      "source": [
        "#!tfds new my_dataset\n",
        "!rm -rf /root/tensorflow_datasets/\n",
        "%cd /content/drive/MyDrive/project_ddg/\n",
        "!tfds build my_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUOGcQw9HmUj"
      },
      "source": [
        "import my_dataset.my_dataset as my_ds\n",
        "import tensorflow_datasets as tfds\n",
        "builder = my_ds.MyDataset()\n",
        "config = tfds.download.DownloadConfig(verify_ssl=False)\n",
        "builder.download_and_prepare(download_config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWFuTtQ_k3qt"
      },
      "source": [
        "#9. VideoClassification on PyTorch!\n",
        "> 구종 자동분류!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imfjj-y0sas3"
      },
      "source": [
        "import os\n",
        "from six.moves import urllib\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "from official.vision.beta.configs import video_classification\n",
        "from official.vision.beta.projects.movinet.configs import movinet as movinet_configs\n",
        "from official.vision.beta.projects.movinet.modeling import movinet\n",
        "from official.vision.beta.projects.movinet.modeling import movinet_layers\n",
        "from official.vision.beta.projects.movinet.modeling import movinet_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8urhTvf-q5rD"
      },
      "source": [
        "movinet_a2_hub_url = 'https://tfhub.dev/tensorflow/movinet/a2/base/kinetics-600/classification/1'\n",
        "\n",
        "inputs = tf.keras.layers.Input(\n",
        "    shape=[None, None, None, 3],\n",
        "    dtype=tf.float32)\n",
        "\n",
        "encoder = hub.KerasLayer(movinet_a2_hub_url, trainable=True)\n",
        "\n",
        "# Important: To use tf.nn.conv3d on CPU, we must compile with tf.function.\n",
        "encoder.call = tf.function(encoder.call, experimental_compile=True)\n",
        "\n",
        "# [batch_size, 600]\n",
        "outputs = encoder(dict(image=inputs))\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgc-hal-tXeJ"
      },
      "source": [
        "num_classes = builder.info.features['label'].num_classes\n",
        "num_examples = {\n",
        "    name: split.num_examples\n",
        "    for name, split in builder.info.splits.items()\n",
        "}\n",
        "\n",
        "print('Number of classes:', num_classes)\n",
        "print('Number of examples for train:', num_examples['train'])\n",
        "print('Number of examples for test:', num_examples['test'])\n",
        "print()\n",
        "\n",
        "builder.info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNE29b4oGQe5"
      },
      "source": [
        "batch_size = 8\n",
        "num_frames = 8\n",
        "frame_stride = 10\n",
        "resolution = 172\n",
        "\n",
        "def format_features(features):\n",
        "  video = features['video']\n",
        "  video = video[:, ::frame_stride]\n",
        "  video = video[:, :num_frames]\n",
        "\n",
        "  video = tf.reshape(video, [-1, video.shape[2], video.shape[3], 3])\n",
        "  video = tf.image.resize(video, (resolution, resolution))\n",
        "  video = tf.reshape(video, [-1, num_frames, resolution, resolution, 3])\n",
        "  video = tf.cast(video, tf.float32) / 255.\n",
        "\n",
        "  label = tf.one_hot(features['label'], num_classes)\n",
        "  return (video, label)\n",
        "\n",
        "train_dataset = builder.as_dataset(\n",
        "    split='train',\n",
        "    batch_size=batch_size,\n",
        "    shuffle_files=True)\n",
        "train_dataset = train_dataset.map(\n",
        "    format_features,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.prefetch(2)\n",
        "\n",
        "test_dataset = builder.as_dataset(\n",
        "    split='test',\n",
        "    batch_size=batch_size)\n",
        "test_dataset = test_dataset.map(\n",
        "    format_features,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    deterministic=True)\n",
        "test_dataset = test_dataset.prefetch(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqPrM2PJDJHU"
      },
      "source": [
        "videos, labels = next(iter(train_dataset))\n",
        "media.show_videos(videos.numpy(), codec='gif', fps=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OcdVx1wAmNk"
      },
      "source": [
        "videos.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLuf0nzlDozS"
      },
      "source": [
        "test_dataset.sample_from_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFlzRos3Fni7"
      },
      "source": [
        "test_dataset.shape()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY3FIUDiBcaj"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9DlAcU0OBHx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHVgjBQZDuCG"
      },
      "source": [
        "model_id = 'a0'\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "backbone = movinet.Movinet(\n",
        "    model_id=model_id)\n",
        "model = movinet_model.MovinetClassifier(\n",
        "    backbone=backbone,\n",
        "    num_classes=2)\n",
        "model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "# Load pretrained weights from TF Hub\n",
        "movinet_hub_url = f'https://tfhub.dev/tensorflow/movinet/{model_id}/base/kinetics-600/classification/1'\n",
        "movinet_hub_model = hub.KerasLayer(movinet_hub_url, trainable=True)\n",
        "pretrained_weights = {w.name: w for w in movinet_hub_model.weights}\n",
        "model_weights = {w.name: w for w in model.weights}\n",
        "for name in pretrained_weights:\n",
        "  model_weights[name].assign(pretrained_weights[name])\n",
        "\n",
        "# Wrap the backbone with a new classifier to create a new classifier head\n",
        "# with num_classes outputs\n",
        "model = movinet_model.MovinetClassifier(\n",
        "    backbone=backbone,\n",
        "    num_classes=num_classes)\n",
        "model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "# Freeze all layers except for the final classifier head\n",
        "for layer in model.layers[:-1]:\n",
        "  layer.trainable = False\n",
        "model.layers[-1].trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z17dcRwWSB7f"
      },
      "source": [
        "batch_size=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEz_FZNStc0Z"
      },
      "source": [
        "num_epochs = 3\n",
        "\n",
        "train_steps = num_examples['train'] // batch_size\n",
        "total_train_steps = train_steps * num_epochs\n",
        "test_steps = num_examples['test'] // batch_size\n",
        "\n",
        "loss_obj = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    label_smoothing=0.1)\n",
        "\n",
        "metrics = [\n",
        "    tf.keras.metrics.TopKCategoricalAccuracy(\n",
        "        k=1, name='top_1', dtype=tf.float32),\n",
        "    tf.keras.metrics.TopKCategoricalAccuracy(\n",
        "        k=5, name='top_5', dtype=tf.float32),\n",
        "]\n",
        "\n",
        "initial_learning_rate = 0.01\n",
        "learning_rate = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate, decay_steps=total_train_steps,\n",
        ")\n",
        "optimizer = tf.keras.optimizers.RMSprop(\n",
        "    learning_rate, rho=0.9, momentum=0.9, epsilon=1.0, clipnorm=1.0)\n",
        "\n",
        "model.compile(loss=loss_obj, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.TensorBoard(),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu2oKy0SqSDY"
      },
      "source": [
        "train_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKCS3s0wRSlj"
      },
      "source": [
        "test_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tyx9_CjIhtC"
      },
      "source": [
        "results = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=num_epochs,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_steps=test_steps,\n",
        "    callbacks=callbacks,\n",
        "    validation_freq=1,\n",
        "    verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps6le0xml0Tm"
      },
      "source": [
        "from keras.utils import tf_utils\n",
        "tf_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX3d8MnDmMcK"
      },
      "source": [
        "logs = tf_utils.sync_to_numpy_or_python_type(logs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzoCK-_8mM3C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzwK_jOvteeR"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs --port 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEwzP6hpninN"
      },
      "source": [
        "#Video Classification with Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-6c0H7amzz3"
      },
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzkwVWRBnrw3"
      },
      "source": [
        "!wget -q https://git.io/JGc31 -O ucf101_top5.tar.gz\n",
        "!tar xf ucf101_top5.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eKwE_slns8l"
      },
      "source": [
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TCmy1lDnuTc"
      },
      "source": [
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOCjBpgWovN1"
      },
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6N3Bm4-nvH1"
      },
      "source": [
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.DenseNet121(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.densenet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_featutes = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[1]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_featutes[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_featutes[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_featutes.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMO2l4Qpovig"
      },
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIZvnbsdpGMI"
      },
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v286Wln0pvcL"
      },
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/content/drive/MyDrive/project_ddg/class_video/CH\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_split=0.15,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(test_data, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSGyAQsvp_bM"
      },
      "source": [
        "trained_model = run_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAQ9SmXCqWjI"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from typing import Callable, List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.sampler import RandomSampler\n",
        "\n",
        "import flash\n",
        "from flash.core.classification import Labels\n",
        "from flash.core.data.utils import download_data\n",
        "from flash.core.finetuning import NoFreeze\n",
        "from flash.core.utilities.imports import _KORNIA_AVAILABLE, _PYTORCHVIDEO_AVAILABLE\n",
        "from flash.video import VideoClassificationData, VideoClassifier\n",
        "\n",
        "if _PYTORCHVIDEO_AVAILABLE and _KORNIA_AVAILABLE:\n",
        "    import kornia.augmentation as K\n",
        "    from pytorchvideo.transforms import ApplyTransformToKey, RandomShortSideScale, UniformTemporalSubsample\n",
        "    from torchvision.transforms import CenterCrop, Compose, RandomCrop, RandomHorizontalFlip\n",
        "else:\n",
        "    print(\"Please, run `pip install torchvideo kornia`\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # 1. Download a video clip dataset. Find more dataset at https://pytorchvideo.readthedocs.io/en/latest/data.html\n",
        " #   download_data(\"https://pl-flash-data.s3.amazonaws.com/kinetics.zip\")\n",
        "\n",
        "    # 2. [Optional] Specify transforms to be used during training.\n",
        "    # Flash helps you to place your transform exactly where you want.\n",
        "    # Learn more at:\n",
        "    # https://lightning-flash.readthedocs.io/en/latest/general/data.html#flash.core.data.process.Preprocess\n",
        "    post_tensor_transform = [UniformTemporalSubsample(8), RandomShortSideScale(min_size=256, max_size=320)]\n",
        "    per_batch_transform_on_device = [K.Normalize(torch.tensor([0.45, 0.45, 0.45]), torch.tensor([0.225, 0.225, 0.225]))]\n",
        "\n",
        "    train_post_tensor_transform = post_tensor_transform + [RandomCrop(244), RandomHorizontalFlip(p=0.5)]\n",
        "    val_post_tensor_transform = post_tensor_transform + [CenterCrop(244)]\n",
        "    train_per_batch_transform_on_device = per_batch_transform_on_device\n",
        "\n",
        "    def make_transform(\n",
        "        post_tensor_transform: List[Callable] = post_tensor_transform,\n",
        "        per_batch_transform_on_device: List[Callable] = per_batch_transform_on_device\n",
        "    ):\n",
        "        return {\n",
        "            \"post_tensor_transform\": Compose([\n",
        "                ApplyTransformToKey(\n",
        "                    key=\"video\",\n",
        "                    transform=Compose(post_tensor_transform),\n",
        "                ),\n",
        "            ]),\n",
        "            \"per_batch_transform_on_device\": Compose([\n",
        "                ApplyTransformToKey(\n",
        "                    key=\"video\",\n",
        "                    transform=K.VideoSequential(\n",
        "                        *per_batch_transform_on_device, data_format=\"BCTHW\", same_on_frame=False\n",
        "                    )\n",
        "                ),\n",
        "            ]),\n",
        "        }\n",
        "\n",
        "    # 3. Load the data from directories.\n",
        "    datamodule = VideoClassificationData.from_folders(\n",
        "        train_folder=os.path.join(flash.PROJECT_ROOT, \"data/kinetics/train\"),\n",
        "        val_folder=os.path.join(flash.PROJECT_ROOT, \"data/kinetics/val\"),\n",
        "        predict_folder=os.path.join(flash.PROJECT_ROOT, \"data/kinetics/predict\"),\n",
        "        train_transform=make_transform(train_post_tensor_transform),\n",
        "        val_transform=make_transform(val_post_tensor_transform),\n",
        "        predict_transform=make_transform(val_post_tensor_transform),\n",
        "        batch_size=8,\n",
        "        clip_sampler=\"uniform\",\n",
        "        clip_duration=1,\n",
        "        video_sampler=RandomSampler,\n",
        "        decode_audio=False,\n",
        "        num_workers=8\n",
        "    )\n",
        "\n",
        "    # 4. List the available models\n",
        "    print(VideoClassifier.available_backbones())\n",
        "    # out: ['efficient_x3d_s', 'efficient_x3d_xs', ... ,slowfast_r50', 'x3d_m', 'x3d_s', 'x3d_xs']\n",
        "    print(VideoClassifier.get_backbone_details(\"x3d_xs\"))\n",
        "\n",
        "    # 5. Build the VideoClassifier with a PyTorchVideo backbone.\n",
        "    model = VideoClassifier(\n",
        "        backbone=\"x3d_xs\", num_classes=datamodule.num_classes, serializer=Labels(), pretrained=False\n",
        "    )\n",
        "\n",
        "    # 6. Finetune the model\n",
        "    trainer = flash.Trainer(fast_dev_run=True)\n",
        "    trainer.finetune(model, datamodule=datamodule, strategy=NoFreeze())\n",
        "\n",
        "    trainer.save_checkpoint(\"video_classification.pt\")\n",
        "\n",
        "    # 7. Make a prediction\n",
        "    predictions = model.predict(os.path.join(flash.PROJECT_ROOT, \"data/kinetics/predict\"))\n",
        "    print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt5TJXTxEBKa"
      },
      "source": [
        "%cd /content/drive/MyDrive\n",
        "#quiet 하위 디렉토리만 \n",
        "\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "%cd /content/drive/MyDrive/models/official/vision/beta/projects/movinet\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3VnNXXZFalU"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from official.vision.beta.projects.movinet.modeling import movinet\n",
        "from official.vision.beta.projects.movinet.modeling import movinet_model\n",
        "\n",
        "# Create backbone and model.\n",
        "backbone = movinet.Movinet(\n",
        "    model_id='a0',\n",
        "    causal=True,\n",
        "    use_external_states=True,\n",
        ")\n",
        "model = movinet_model.MovinetClassifier(\n",
        "    backbone, num_classes=600, output_states=True)\n",
        "\n",
        "# Create your example input here.\n",
        "# Refer to the paper for recommended input shapes.\n",
        "inputs = tf.ones([1, 8, 172, 172, 3])\n",
        "\n",
        "# [Optional] Build the model and load a pretrained checkpoint\n",
        "model.build(inputs.shape)\n",
        "\n",
        "checkpoint_dir = '/path/to/checkpoint'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "status.assert_existing_objects_matched()\n",
        "\n",
        "# Run the model prediction.\n",
        "output = model(inputs)\n",
        "prediction = tf.argmax(output, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3uP6r2vECAx"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from official.vision.beta.projects.movinet.modeling import movinet\n",
        "from official.vision.beta.projects.movinet.modeling import movinet_model\n",
        "\n",
        "# Create backbone and model.\n",
        "backbone = movinet.Movinet(\n",
        "    model_id='a0',\n",
        "    causal=True,\n",
        "    use_external_states=True,\n",
        ")\n",
        "model = movinet_model.MovinetClassifier(\n",
        "    backbone, num_classes=7, output_states=True)\n",
        "\n",
        "# Create your example input here.\n",
        "# Refer to the paper for recommended input shapes.\n",
        "inputs = tf.ones([1, 8, 172, 172, 3])\n",
        "\n",
        "# [Optional] Build the model and load a pretrained checkpoint\n",
        "model.build(inputs.shape)\n",
        "\n",
        "checkpoint_dir = '/path/to/checkpoint'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "status.assert_existing_objects_matched()\n",
        "\n",
        "# Run the model prediction.\n",
        "output = model(inputs)\n",
        "prediction = tf.argmax(output, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xWTL-6ZVp2E"
      },
      "source": [
        "!pip install av\n",
        "! wget https://raw.githubusercontent.com/pytorch/vision/6de158c473b83cf43344a0651d7c01128c7850e6/references/video_classification/transforms.py\n",
        "# Download HMDB51 data and splits from serre lab website\n",
        "! wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n",
        "! wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/test_train_splits.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pgsVKZaLdov"
      },
      "source": [
        "pip install git+https://github.com/Atze00/MoViNet-pytorch.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO37LnX8Ljqj"
      },
      "source": [
        "# Extract and organize video data..\n",
        "! mkdir -p video_data test_train_splits\n",
        "! unrar e test_train_splits.rar test_train_splits\n",
        "! rm test_train_splits.rar\n",
        "! unrar e hmdb51_org.rar \n",
        "! rm hmdb51_org.rar\n",
        "! mv *.rar video_data\n",
        "import os\n",
        "for files in os.listdir('video_data'):\n",
        "    foldername = files.split('.')[0]\n",
        "    os.system(\"mkdir -p video_data/\" + foldername)\n",
        "    os.system(\"unrar e video_data/\"+ files + \" video_data/\"+foldername)\n",
        "\n",
        "! rm video_data/*.rar\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbgJJAAMLzdx"
      },
      "source": [
        "import cv2     # for capturing videos\n",
        "import math   # for mathematical operations\n",
        "import matplotlib.pyplot as plt    # for plotting the images\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from keras.preprocessing import image   # for preprocessing the images\n",
        "import numpy as np    # for mathematical operations\n",
        "from keras.utils import np_utils\n",
        "from skimage.transform import resize   # for resizing images\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPFJtyfbWVwF"
      },
      "source": [
        "# open the .txt file which have names of training videos\n",
        "f = open(\"trainlist01.txt\", \"r\")\n",
        "temp = f.read()\n",
        "videos = temp.split('\\n')\n",
        "\n",
        "# creating a dataframe having video names\n",
        "train = pd.DataFrame()\n",
        "train['video_name'] = videos\n",
        "train = train[:-1]\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqHTKMjyWZfb"
      },
      "source": [
        "import cv2     # for capturing videos\n",
        "import math   # for mathematical operations\n",
        "import matplotlib.pyplot as plt    # for plotting the images\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from keras.preprocessing import image   # for preprocessing the images\n",
        "import numpy as np    # for mathematical operations\n",
        "from keras.utils import np_utils\n",
        "from skimage.transform import resize   # for resizing images\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWyDzFSvSqZy"
      },
      "source": [
        "!unrar e UCF101.rar Videos/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7id0glhSfMN"
      },
      "source": [
        "# open the .txt file which have names of training videos\n",
        "f = open(\"trainlist01.txt\", \"r\")\n",
        "temp = f.read()\n",
        "videos = temp.split('\\n')\n",
        "\n",
        "# creating a dataframe having video names\n",
        "train = pd.DataFrame()\n",
        "train['video_name'] = videos\n",
        "train = train[:-1]\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTnhLyNCT_79"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/project_ddg/data3/train_labels.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/project_ddg/data3/test_labels.csv')\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtc9bABNUGqW"
      },
      "source": [
        "df_train=pd.DataFrame(train['video_id'].str.replace(\".\",\".avi\"))\n",
        "df_test=pd.DataFrame(test['video_id'].str.replace(\".\",\".avi\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6F3UzDAgxzZ"
      },
      "source": [
        "test_videos = test['video_id']\n",
        "test_videos\n",
        "y = df_train['video_id']\n",
        "y = pd.get_dummies(y)\n",
        "y\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCZ_UmHKlqsX"
      },
      "source": [
        "predict = []\n",
        "actual = []\n",
        "\n",
        "# for loop to extract frames from each test video\n",
        "for i in tqdm(range(test_videos.shape[0])):\n",
        "    count = 0\n",
        "    videoFile = test_videos[i]\n",
        "    cap = cv2.VideoCapture('content/drive/MyDrive/project_ddg/data3/train_1/'+3\n",
        "                           .split(' ')[0].split('/')[1])   # capturing the video from the given path\n",
        "    frameRate = cap.get(5) #frame rate\n",
        "    x=1\n",
        "    # removing all other files from the temp folder\n",
        "    files = glob('temp/*')\n",
        "    for f in files:\n",
        "        os.remove(f)\n",
        "    while(cap.isOpened()):\n",
        "        frameId = cap.get(1) #current frame number\n",
        "        ret, frame = cap.read()\n",
        "        if (ret != True):\n",
        "            break\n",
        "        if (frameId % math.floor(frameRate) == 0):\n",
        "            # storing the frames of this particular video in temp folder\n",
        "            filename ='temp/' + \"_frame%d.jpg\" % count;count+=1\n",
        "            cv2.imwrite(filename, frame)\n",
        "    cap.release()\n",
        "    \n",
        "    # reading all the frames from temp folder\n",
        "    images = glob(\"temp/*.jpg\")\n",
        "    \n",
        "    prediction_images = []\n",
        "    for i in range(len(images)):\n",
        "        img = image.load_img(images[i], target_size=(224,224,3))\n",
        "        img = image.img_to_array(img)\n",
        "        img = img/255\n",
        "        prediction_images.append(img)\n",
        "        \n",
        "    # converting all the frames for a test video into numpy array\n",
        "    prediction_images = np.array(prediction_images)\n",
        "    # extracting features using pre-trained model\n",
        "    prediction_images = base_model.predict(prediction_images)\n",
        "    # converting features in one dimensional array\n",
        "    prediction_images = prediction_images.reshape(prediction_images.shape[0], 7*7*512)\n",
        "    # predicting tags for each array\n",
        "    prediction = model.predict_classes(prediction_images)\n",
        "    # appending the mode of predictions in predict list to assign the tag to the video\n",
        "    predict.append(y.columns.values[s.mode(prediction)[0][0]])\n",
        "    # appending the actual tag of the video\n",
        "    actual.append(videoFile.split('/')[1].split('_')[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lSqkPFlUq6h"
      },
      "source": [
        "# storing the frames from training videos\n",
        "for i in tqdm(range(train.shape[0])):\n",
        "    count = 0\n",
        "    videoFile = train['video_id'][i]\n",
        "    cap = cv2.VideoCapture('/content/drive/MyDrive/project_ddg/data3/train/'+videoFile+\"mp4\")   # capturing the video from the given path\n",
        "    frameRate = cap.get(60) #frame rate\n",
        "    x=1\n",
        "    while(cap.isOpened()):\n",
        "        frameId = cap.get(1) #current frame number\n",
        "        ret, frame = cap.read()\n",
        "        if (ret != True):\n",
        "            break\n",
        "        if (frameId % math.floor(frameRate) == 0):\n",
        "            # storing the frames in a new folder named train_1\n",
        "            filename ='/content/drive/MyDrive/project_ddg/data3/train_1/' + videoFile + '_frame%d.jpg' % count;count+=1\n",
        "            cv2.imwrite(filename, frame)\n",
        "    cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9e6xk40YHMY"
      },
      "source": [
        "# getting the names of all the images\n",
        "images = glob(\"/content/drive/MyDrive/project_ddg/data3/train_1/*.jpg\")\n",
        "train_image = []\n",
        "train_class = []\n",
        "for i in tqdm(range(len(images))):\n",
        "    # creating the image name\n",
        "    train_image.append(images[i].split('/')[1])\n",
        "    # creating the class of image\n",
        "    train_class.append(images[i].split('/')[1].split('_')[1])\n",
        "    \n",
        "# storing the images and their class in a dataframe\n",
        "train_data = pd.DataFrame()\n",
        "train_data['image'] = train_image\n",
        "train_data['class'] = train_class\n",
        "\n",
        "# converting the dataframe into csv file \n",
        "train_data.to_csv('UCF/train_new.csv',header=True, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q720TybCed4A"
      },
      "source": [
        "# getting the test list\n",
        "test_videos = test['video_name']\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SR-ztT8eeXZ"
      },
      "source": [
        "train=pd.read_csv(\"/content/drive/MyDrive/project_ddg/data3/train_labels.csv\")\n",
        "test=pd.read_csv(\"/content/drive/MyDrive/project_ddg/data3/test_labels.csv\")\n",
        "\n",
        "test_videos= train['video_id']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gEiczTLncfT"
      },
      "source": [
        "!pip install tensorflow_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hNTj8LbrKyJ"
      },
      "source": [
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azr6f5FjrPU7"
      },
      "source": [
        "#CNN RNN by Keras\n",
        "> https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/video_classification.ipynb#scrollTo=j4dyJCWMQgrM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecSexTHknc1A"
      },
      "source": [
        "\n",
        "#from tensorflow_docs.vis import embed\n",
        "from tensorflow import keras\n",
        "from imutils import paths\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q2Ev5V7nfrS"
      },
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "MAX_SEQ_LENGTH = 40\n",
        "NUM_FEATURES = 2048"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqkEIhNEnqJi"
      },
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/project_ddg/video_dataset/train/train_labels.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/project_ddg/video_dataset/test/test_labels.csv\")\n",
        "#train_df['path'] = train_df['path']+str(\".mp4\")\n",
        "#test_df['path'] = test_df['path']+str(\".mp4\")\n",
        "\n",
        "train_df.columns=[\"video_name\",\"tag\"]\n",
        "test_df.columns=[\"video_name\",\"tag\"]\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajZpWXFQnv6L"
      },
      "source": [
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNcpY7ZTn0nS"
      },
      "source": [
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKe2IS0KoORA"
      },
      "source": [
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
        ")\n",
        "print(label_processor.get_vocabulary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXqeDyOFHtVU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBU2b0G0DHvz"
      },
      "source": [
        "\n",
        "# Initialize placeholders to store the masks and features of the current video.\n",
        "temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "temp_frame_featutes = np.zeros(\n",
        "    shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "# Extract features from the frames of the current video.\n",
        "for i, batch in enumerate(ab):\n",
        "    video_length = batch.shape[0]\n",
        "    length = min(MAX_SEQ_LENGTH, video_length)\n",
        "    for j in range(length):\n",
        "        temp_frame_featutes[i, j, :] = feature_extractor.predict(\n",
        "            batch[None, j, :]\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyqO9bQKoSXI"
      },
      "source": [
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    # masked with padding or not.\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "        temp_frame_featutes = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                temp_frame_featutes[i, j, :] = feature_extractor.predict(\n",
        "                    batch[None, j, :]\n",
        "                )\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        frame_features[idx,] = temp_frame_featutes.squeeze()\n",
        "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "\n",
        "train_data, train_labels = prepare_all_videos(train_df, \"/content/drive/MyDrive/project_ddg/video_dataset/train/\")\n",
        "test_data, test_labels = prepare_all_videos(test_df, \"/content/drive/MyDrive/project_ddg/video_dataset/test/\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7lreNw7THPJ"
      },
      "source": [
        "# Utility for our sequence model.\n",
        "def get_sequence_model():\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
        "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "\n",
        "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
        "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
        "    x = keras.layers.GRU(16, return_sequences=True)(\n",
        "        frame_features_input, mask=mask_input\n",
        "    )\n",
        "    x = keras.layers.GRU(8)(x)\n",
        "    x = keras.layers.Dropout(0.4)(x)\n",
        "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
        "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
        "\n",
        "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
        "\n",
        "    rnn_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return rnn_model\n",
        "\n",
        "\n",
        "# Utility for running experiments.\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    seq_model = get_sequence_model()\n",
        "    history = seq_model.fit(\n",
        "        [train_data[0], train_data[1]],\n",
        "        train_labels,\n",
        "        validation_split=0.3,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    seq_model.load_weights(filepath)\n",
        "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, seq_model\n",
        "\n",
        "\n",
        "_, sequence_model = run_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5q3ESMoTR0q"
      },
      "source": [
        "\n",
        "def prepare_single_video(frames):\n",
        "    frames = frames[None, ...]\n",
        "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "    frame_featutes = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "\n",
        "    for i, batch in enumerate(frames):\n",
        "        video_length = batch.shape[0]\n",
        "        length = min(MAX_SEQ_LENGTH, video_length)\n",
        "        for j in range(length):\n",
        "            frame_featutes[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "    return frame_featutes, frame_mask\n",
        "\n",
        "\n",
        "def sequence_prediction(path):\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frames = load_video(os.path.join(\"test\", path))\n",
        "    frame_features, frame_mask = prepare_single_video(frames)\n",
        "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
        "\n",
        "    for i in np.argsort(probabilities)[::-1]:\n",
        "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
        "    return frames\n",
        "\n",
        "\n",
        "# This utility is for visualization.\n",
        "# Referenced from:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def to_gif(images):\n",
        "    converted_images = images.astype(np.uint8)\n",
        "    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n",
        "    return embed.embed_file(\"animation.gif\")\n",
        "\n",
        "\n",
        "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
        "print(f\"Test video path: {test_video}\")\n",
        "test_frames = sequence_prediction(test_video)\n",
        "to_gif(test_frames[:MAX_SEQ_LENGTH])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLr4__au9siv"
      },
      "source": [
        "#Video Classification with Transformers\n",
        "> https://keras.io/examples/vision/video_transformers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z2X35k09NqW"
      },
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtiEPFPI9vSQ"
      },
      "source": [
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO7qicHG9zS3"
      },
      "source": [
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg4AYJqInWXg"
      },
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/project_ddg/video_dataset/train/train_labels.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/project_ddg/video_dataset/test/test_labels.csv\")\n",
        "#train_df['video_id'] = train_df['video_id']+str(\".avi\")\n",
        "#test_df['video_id'] = test_df['video_id']+str(\".avi\")\n",
        "\n",
        "train_df.columns=[\"video_name\",\"tag\"]\n",
        "test_df.columns=[\"video_name\",\"tag\"]\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s76YWOSOAjOY"
      },
      "source": [
        "\n",
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.DenseNet121(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.densenet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_featutes = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[1]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_featutes[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_featutes[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_featutes.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDUwNUjGAsU4"
      },
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV_P4vgeAu9v"
      },
      "source": [
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLGbk5DEJLH-"
      },
      "source": [
        "# Dataset Loader by Pytorch\n",
        ">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0jbugpZOpWo"
      },
      "source": [
        "!git clone https://github.com/YuxinZhaozyx/pytorch-VideoDataset.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbvqbD2zPhdF"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/pytorch-VideoDataset\")\n",
        "sys.path\n",
        "\n",
        "import datasets\n",
        "import transforms\n",
        "import torch\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uINMJmhSeKe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjWa9vwgVRBx"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/project_ddg/data4/train_.csv\")\n",
        "#df.columns = ['path','label']\n",
        "df['path'] =  str(\"/content/drive/MyDrive/project_ddg/data4/train/\") + df['path']+ str(\".mp4\")\n",
        "#df.to_csv(\"/content/drive/MyDrive/project_ddg/data4/train_.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbZhCMM8agKx"
      },
      "source": [
        "\n",
        "\n",
        "dataset = datasets.VideoLabelDataset(\n",
        "\t\"/content/drive/MyDrive/project_ddg/data4/train_.csv\",\n",
        "    transform=torchvision.transforms.Compose([\n",
        "        transforms.VideoFilePathToTensor(max_len=50, fps=10, padding_mode='last'),\n",
        "        transforms.VideoRandomCrop([512, 512]),\n",
        "        transforms.VideoResize([256, 256]),\n",
        "    ])\n",
        ")\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size = 2, shuffle = True)\n",
        "for videos, labels in data_loader:\n",
        "    print(videos.size(), labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6_lfBSW6eua"
      },
      "source": [
        "data_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgl1Hzjmay7-"
      },
      "source": [
        "!pip install git+https://github.com/Atze00/MoViNet-pytorch.git\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import torch\n",
        "import transforms as T\n",
        "from movinets import MoViNet\n",
        "from movinets.config import _C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI_Mgbtaa3CZ"
      },
      "source": [
        "def train_iter(model, optimz, data_load, loss_val):\n",
        "    samples = len(data_load.dataset)\n",
        "    model.train()\n",
        "    model.cuda()\n",
        "    model.clean_activation_buffers()\n",
        "    optimz.zero_grad()\n",
        "    for i, (data,_ , target) in enumerate(data_load):\n",
        "        out = F.log_softmax(model(data.cuda()), dim=1)\n",
        "        loss = F.nll_loss(out, target.cuda())\n",
        "        loss.backward()\n",
        "        optimz.step()\n",
        "        optimz.zero_grad()\n",
        "        model.clean_activation_buffers()\n",
        "        if i % 50 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_load)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_val.append(loss.item())\n",
        " \n",
        "def evaluate(model, data_load, loss_val):\n",
        "    model.eval()\n",
        "    \n",
        "    samples = len(data_load.dataset)\n",
        "    csamp = 0\n",
        "    tloss = 0\n",
        "    model.clean_activation_buffers()\n",
        "    with torch.no_grad():\n",
        "        for data, _, target in data_load:\n",
        "            output = F.log_softmax(model(data.cuda()), dim=1)\n",
        "            loss = F.nll_loss(output, target.cuda(), reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            \n",
        "            tloss += loss.item()\n",
        "            csamp += pred.eq(target.cuda()).sum()\n",
        "            model.clean_activation_buffers()\n",
        "    aloss = tloss / samples\n",
        "    loss_val.append(aloss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(aloss) +\n",
        "          '  Accuracy:' + '{:5}'.format(csamp) + '/' +\n",
        "          '{:5}'.format(samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * csamp / samples) + '%)\\n')\n",
        "    \n",
        "def train_iter_stream(model, optimz, data_load, loss_val, n_clips = 2, n_clip_frames=8):\n",
        "    \"\"\"\n",
        "    In causal mode with stream buffer a single video is fed to the network\n",
        "    using subclips of lenght n_clip_frames. \n",
        "    n_clips*n_clip_frames should be equal to the total number of frames presents\n",
        "    in the video.\n",
        "    \n",
        "    n_clips : number of clips that are used\n",
        "    n_clip_frames : number of frame contained in each clip\n",
        "    \"\"\"\n",
        "    #clean the buffer of activations\n",
        "    samples = len(data_load.dataset)\n",
        "    model.cuda()\n",
        "    model.train()\n",
        "    model.clean_activation_buffers()\n",
        "    optimz.zero_grad()\n",
        "    \n",
        "    for i, (data,_, target) in enumerate(data_load):\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "        l_batch = 0\n",
        "        #backward pass for each clip\n",
        "        for j in range(n_clips):\n",
        "          output = F.log_softmax(model(data[:,:,(n_clip_frames)*(j):(n_clip_frames)*(j+1)]), dim=1)\n",
        "          loss = F.nll_loss(output, target)\n",
        "          _, pred = torch.max(output, dim=1)\n",
        "          loss = F.nll_loss(output, target)/n_clips\n",
        "          loss.backward()\n",
        "        l_batch += loss.item()*n_clips\n",
        "        optimz.step()\n",
        "        optimz.zero_grad()\n",
        "        \n",
        "        #clean the buffer of activations\n",
        "        model.clean_activation_buffers()\n",
        "        if i % 50 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_load)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(l_batch))\n",
        "            loss_val.append(l_batch)\n",
        "\n",
        "def evaluate_stream(model, data_load, loss_val, n_clips = 2, n_clip_frames=8):\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "    samples = len(data_load.dataset)\n",
        "    csamp = 0\n",
        "    tloss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _, target in data_load:\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "            model.clean_activation_buffers()\n",
        "            for j in range(n_clips):\n",
        "              output = F.log_softmax(model(data[:,:,(n_clip_frames)*(j):(n_clip_frames)*(j+1)]), dim=1)\n",
        "              loss = F.nll_loss(output, target)\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            tloss += loss.item()\n",
        "            csamp += pred.eq(target).sum()\n",
        "\n",
        "    aloss = tloss /  len(data_load)\n",
        "    loss_val.append(aloss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(aloss) +\n",
        "          '  Accuracy:' + '{:5}'.format(csamp) + '/' +\n",
        "          '{:5}'.format(samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * csamp / samples) + '%)\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bguIxIo8gyyY"
      },
      "source": [
        "import time\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import torch\n",
        "import transforms as T\n",
        "from movinets import MoViNet\n",
        "from movinets.config import _C\n",
        "\n",
        "torch.manual_seed(97)\n",
        "num_frames = 16 # 16\n",
        "clip_steps = 2\n",
        "Bs_Train = 1\n",
        "Bs_Test = 2\n",
        "\n",
        "train_loader = DataLoader(data_loader, batch_size=Bs_Train, shuffle=True)\n",
        "train_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mYzqVSngkDj"
      },
      "source": [
        "N_EPOCHS = 1\n",
        "\n",
        "model = MoViNet(_C.MODEL.MoViNetA0, causal = True, pretrained = True )\n",
        "start_time = time.time()\n",
        "\n",
        "trloss_val, tsloss_val = [], []\n",
        "model.classifier[3] = torch.nn.Conv3d(2048, 51, (1,1,1))\n",
        "optimz = optim.Adam(model.parameters(), lr=0.00005)\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    train_iter_stream(model, optimz, data_loader, trloss_val)\n",
        "    evaluate_stream(model, test_loader, tsloss_val)\n",
        " \n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUzU1j5sgmki"
      },
      "source": [
        "train_iter_stream(model, optimz, data_loader, trloss_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnnC0eyjizlx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}